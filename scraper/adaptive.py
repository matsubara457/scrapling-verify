"""Adaptive ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Scraplingã® Adaptive æ©Ÿèƒ½ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
Phase1: v1ã®HTMLã§auto_save=Trueã«ã‚ˆã‚Šè¦ç´ ã®æŒ‡ç´‹ã‚’ä¿å­˜
Phase2: v2ã®HTMLã§adaptive=Trueã«ã‚ˆã‚Šå¾©å…ƒï¼ˆBS4ã¨ã®æ¯”è¼ƒã‚ã‚Šï¼‰
"""

import json
import os
import sys

import requests
from bs4 import BeautifulSoup
from scrapling.fetchers import Fetcher
from scrapling.parser import Selector

BASE_URL = "http://localhost:5001"
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")

SELECTORS = [
    (".product-name", "å•†å“å"),
    (".product-price", "ä¾¡æ ¼"),
    (".product-rating", "è©•ä¾¡"),
    (".product-category", "ã‚«ãƒ†ã‚´ãƒª"),
    (".product-desc", "èª¬æ˜"),
]


def get_version(url: str = BASE_URL) -> str:
    resp = requests.get(f"{url}/version")
    return resp.json()["version"]


def switch_version(url: str = BASE_URL) -> str:
    """ã‚µã‚¤ãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶­æŒã®ãŸã‚åŒã˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ï¼‰"""
    session = requests.Session()
    session.get(f"{url}/switch")
    resp = session.get(f"{url}/version")
    return resp.json()["version"]


def phase1_save(url: str = BASE_URL) -> dict:
    """v1ã®HTMLã§è¦ç´ ã®æŒ‡ç´‹ã‚’ä¿å­˜ã™ã‚‹"""
    page = Fetcher.get(url)
    html = page.html_content
    selector = Selector(html, url=url, auto_save=True)

    results = {}
    for css, label in SELECTORS:
        found = selector.css(css)
        if found:
            results[label] = found[0].text.strip()
            print(f"  ä¿å­˜: {label} ({css}) â†’ ã€Œ{results[label]}ã€")
        else:
            results[label] = None
            print(f"  æœªæ¤œå‡º: {label} ({css})")

    return results


def phase2_restore(url: str = BASE_URL) -> dict:
    """v2ã®HTMLã§adaptiveå¾©å…ƒã‚’è©¦ã¿ã‚‹ï¼ˆBS4ã¨ã®æ¯”è¼ƒã‚ã‚Šï¼‰"""
    page = Fetcher.get(url)
    html = page.html_content

    # BS4ã§v1ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
    bs4_results = _bs4_check(html)

    # Scrapling Adaptiveã§å¾©å…ƒ
    selector = Selector(html, url=url, auto_save=True)
    scrapling_results = {}

    for css, label in SELECTORS:
        found = selector.css(css)
        if found:
            el = found[0]
            scrapling_results[label] = {
                "text": el.text.strip(),
                "tag": el.tag,
                "class": el.attrib.get("class", ""),
                "original_selector": css,
                "status": "restored",
            }
            print(f"  å¾©å…ƒæˆåŠŸ: {label} ({css}) â†’ <{el.tag} class=\"{el.attrib.get('class', '')}\"> ã€Œ{el.text.strip()}ã€")
        else:
            scrapling_results[label] = {
                "status": "not_found",
                "original_selector": css,
            }
            print(f"  å¾©å…ƒå¤±æ•—: {label} ({css})")

    return {"bs4": bs4_results, "scrapling": scrapling_results}


def _bs4_check(html: str) -> dict:
    """BS4ã§v1ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œã—ã€ãƒ’ãƒƒãƒˆæ•°ã‚’è¿”ã™"""
    soup = BeautifulSoup(html, "html.parser")
    results = {}
    for css, label in SELECTORS:
        class_name = css.lstrip(".")
        count = len(soup.find_all(class_=class_name))
        results[label] = count
        status = f"âœ… {count}ä»¶" if count > 0 else "ğŸ’¥ 0ä»¶"
        print(f"  BS4: {label} ({css}) â†’ {status}")
    return results


def run_full_demo(url: str = BASE_URL) -> dict:
    """ãƒ•ãƒ«ãƒ‡ãƒ¢: v1ä¿å­˜ â†’ v2åˆ‡æ›¿ â†’ å¾©å…ƒ â†’ æ¯”è¼ƒ"""
    print("=" * 50)
    print("Adaptive Scraping ãƒ•ãƒ«ãƒ‡ãƒ¢")
    print("=" * 50)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦v1ã‚’ç¢ºèª
    session = requests.Session()
    resp = session.get(f"{url}/version")
    current = resp.json()["version"]

    # v1ã§ãªã‘ã‚Œã°åˆ‡ã‚Šæ›¿ãˆ
    if current != "v1":
        print(f"\nç¾åœ¨ {current} â†’ v1 ã«åˆ‡æ›¿ä¸­...")
        session.get(f"{url}/switch")

    # Phase1: v1ã§ä¿å­˜
    print(f"\n--- Phase 1: v1ã§æŒ‡ç´‹ä¿å­˜ ---")
    v1_results = phase1_save(url)

    # v2ã«åˆ‡æ›¿
    print(f"\nv2ã«åˆ‡æ›¿ä¸­...")
    session.get(f"{url}/switch")

    # Phase2: v2ã§å¾©å…ƒ
    print(f"\n--- Phase 2: v2ã§Adaptiveå¾©å…ƒ ---")
    print(f"\n[BS4ã§v1ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ]")
    v2_results = phase2_restore(url)

    # çµæœã‚’ã¾ã¨ã‚ã¦ä¿å­˜
    result = {
        "v1_save": v1_results,
        "v2_bs4": v2_results["bs4"],
        "v2_scrapling": v2_results["scrapling"],
    }

    filepath = os.path.join(DATA_DIR, "adaptive_result.json")
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nçµæœã‚’ä¿å­˜: {filepath}")

    # ã‚µãƒãƒª
    bs4_found = sum(1 for v in v2_results["bs4"].values() if v > 0)
    scrapling_found = sum(1 for v in v2_results["scrapling"].values() if isinstance(v, dict) and v.get("status") == "restored")
    print(f"\n{'=' * 50}")
    print(f"çµæœã‚µãƒãƒª:")
    print(f"  BS4 (v1ã‚»ãƒ¬ã‚¯ã‚¿ â†’ v2):       {bs4_found}/{len(SELECTORS)} ä»¶æ¤œå‡º")
    print(f"  Scrapling Adaptive (å¾©å…ƒ):    {scrapling_found}/{len(SELECTORS)} ä»¶å¾©å…ƒ")
    print(f"{'=' * 50}")

    # v1ã«æˆ»ã™
    session.get(f"{url}/switch")

    return result


def main():
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python -m scraper.adaptive [phase1|phase2|full]")
        print("  phase1 - v1ã®HTMLã§æŒ‡ç´‹ã‚’ä¿å­˜")
        print("  phase2 - v2ã®HTMLã§Adaptiveå¾©å…ƒ")
        print("  full   - ãƒ•ãƒ«ãƒ‡ãƒ¢ï¼ˆv1ä¿å­˜ â†’ v2å¾©å…ƒ â†’ æ¯”è¼ƒï¼‰")
        sys.exit(1)

    command = sys.argv[1]

    try:
        requests.get(f"{BASE_URL}/version")
    except requests.ConnectionError:
        print("ã‚¨ãƒ©ãƒ¼: Flaskã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« python demo_site/app.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    if command == "phase1":
        print("Phase 1: v1ã§æŒ‡ç´‹ä¿å­˜")
        phase1_save()
    elif command == "phase2":
        print("Phase 2: v2ã§Adaptiveå¾©å…ƒ")
        phase2_restore()
    elif command == "full":
        run_full_demo()
    else:
        print(f"ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {command}")
        sys.exit(1)


if __name__ == "__main__":
    main()
