"""Streamlit ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ â€” Scrapling Price Tracker

å•†å“ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã€Adaptiveæ¯”è¼ƒã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚’æä¾›ã™ã‚‹ã€‚
"""

import glob
import json
import os
import subprocess

import pandas as pd
import plotly.express as px
import streamlit as st

st.set_page_config(
    page_title="Scrapling Price Tracker",
    page_icon="ğŸ•·ï¸",
    layout="wide",
)

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
PROJECT_ROOT = os.path.join(os.path.dirname(__file__), "..")


# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ---
st.sidebar.title("ğŸ•·ï¸ Scrapling Price Tracker")
page = st.sidebar.radio(
    "ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³",
    ["ğŸ  æ¦‚è¦", "ğŸ“Š å•†å“ãƒ‡ãƒ¼ã‚¿", "ğŸ”„ Adaptiveæ¯”è¼ƒ", "âš¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"],
)


# ===== ğŸ  æ¦‚è¦ãƒšãƒ¼ã‚¸ =====
if page == "ğŸ  æ¦‚è¦":
    st.title("ğŸ•·ï¸ Scrapling Price Tracker")
    st.markdown("""
    **Scraplingã®ä¸»è¦æ©Ÿèƒ½ã‚’ãƒ‡ãƒ¢ã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ã§ã™ã€‚
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ€ãƒŸãƒ¼ECã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
    """)

    st.subheader("ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£")
    st.code("""
    [Flask ãƒ€ãƒŸãƒ¼ã‚µã‚¤ãƒˆ]  â†’  [Scrapling ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼]  â†’  [JSON/CSV ãƒ•ã‚¡ã‚¤ãƒ«]
     localhost:5001            Fetcher + Parser              data/
                               Adaptiveæ©Ÿèƒ½
                                      â†“
                              [Streamlit ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰]
                               localhost:8501
    """, language="text")

    st.subheader("ä½¿ã„æ–¹")
    st.markdown("""
    1. **Flaskèµ·å‹•**: `python demo_site/app.py` (port 5001)
    2. **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ**: `python -m scraper.basic` ã¾ãŸã¯å³ã®ã€Œå®Ÿè¡Œã€ãƒšãƒ¼ã‚¸ã‹ã‚‰
    3. **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèª**: ã“ã®ãƒšãƒ¼ã‚¸ã®å„ã‚¿ãƒ–ã‚’ç¢ºèª
    """)

    st.subheader("Scraplingã¨ã¯")
    st.markdown("""
    [Scrapling](https://github.com/D4Vinci/Scrapling) ã¯ GitHub â˜…17,700+ ã®Pythonè£½Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

    **ä¸»ãªç‰¹å¾´:**
    - **Adaptive Scraping**: ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰ã‚ã£ã¦ã‚‚è¦ç´ ã‚’è‡ªå‹•è¿½è·¡
    - **é«˜é€Ÿãƒ‘ãƒ¼ã‚µãƒ¼**: lxmlãƒ™ãƒ¼ã‚¹ã§é«˜é€ŸãªHTMLè§£æ
    - **find_similar()**: é¡ä¼¼è¦ç´ ã®è‡ªå‹•æ¤œå‡º
    - **Fetcher**: httpx/Playwright/Camoufoxã«ã‚ˆã‚‹æŸ”è»ŸãªHTTPå–å¾—
    """)


# ===== ğŸ“Š å•†å“ãƒ‡ãƒ¼ã‚¿ãƒšãƒ¼ã‚¸ =====
elif page == "ğŸ“Š å•†å“ãƒ‡ãƒ¼ã‚¿":
    st.title("ğŸ“Š å•†å“ãƒ‡ãƒ¼ã‚¿")

    json_files = sorted(glob.glob(os.path.join(DATA_DIR, "products_*.json")))

    if not json_files:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€Œâš¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã€ãƒšãƒ¼ã‚¸ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        file_names = [os.path.basename(f) for f in json_files]
        selected = st.selectbox("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹", file_names)
        selected_path = os.path.join(DATA_DIR, selected)

        with open(selected_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        df = pd.DataFrame(data)
        st.subheader("å•†å“ä¸€è¦§")
        st.dataframe(df, use_container_width=True)

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ä¾¡æ ¼æ¯”è¼ƒ")
            if "name" in df.columns and "price" in df.columns:
                fig_bar = px.bar(
                    df, x="name", y="price",
                    title="å•†å“åˆ¥ä¾¡æ ¼",
                    labels={"name": "å•†å“å", "price": "ä¾¡æ ¼ (å††)"},
                    color="price",
                    color_continuous_scale="Blues",
                )
                fig_bar.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig_bar, use_container_width=True)

        with col2:
            st.subheader("ã‚«ãƒ†ã‚´ãƒªåˆ¥å•†å“æ•°")
            if "category" in df.columns:
                category_counts = df["category"].value_counts().reset_index()
                category_counts.columns = ["category", "count"]
                fig_pie = px.pie(
                    category_counts, values="count", names="category",
                    title="ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ",
                )
                st.plotly_chart(fig_pie, use_container_width=True)

        # CSV DL
        csv_data = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=selected.replace(".json", ".csv"),
            mime="text/csv",
        )


# ===== ğŸ”„ Adaptiveæ¯”è¼ƒãƒšãƒ¼ã‚¸ =====
elif page == "ğŸ”„ Adaptiveæ¯”è¼ƒ":
    st.title("ğŸ”„ Adaptive Scraping æ¯”è¼ƒ")

    adaptive_path = os.path.join(DATA_DIR, "adaptive_result.json")

    if not os.path.exists(adaptive_path):
        st.warning("AdaptiveçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«Adaptiveãƒ‡ãƒ¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.code("python -m scraper.adaptive full", language="bash")

        if st.button("ğŸ”„ Adaptive ãƒ•ãƒ«ãƒ‡ãƒ¢å®Ÿè¡Œ"):
            with st.spinner("Adaptive ãƒ•ãƒ«ãƒ‡ãƒ¢å®Ÿè¡Œä¸­..."):
                result = subprocess.run(
                    ["python", "-m", "scraper.adaptive", "full"],
                    capture_output=True, text=True, cwd=PROJECT_ROOT,
                )
                if result.returncode == 0:
                    st.success("å®Ÿè¡Œå®Œäº†ï¼ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                    st.code(result.stdout)
                    st.rerun()
                else:
                    st.error("å®Ÿè¡Œã‚¨ãƒ©ãƒ¼")
                    st.code(result.stderr)
    else:
        with open(adaptive_path, "r", encoding="utf-8") as f:
            adaptive_data = json.load(f)

        # v1 â†’ v2 ã®å¤‰æ›´ç‚¹
        st.subheader("v1 â†’ v2 ã®å¤‰æ›´ç‚¹")
        changes = pd.DataFrame([
            {"è¦ç´ ": "å•†å“ã‚«ãƒ¼ãƒ‰", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "div.product-card", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "article.item-tile"},
            {"è¦ç´ ": "å•†å“å", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "h2.product-name", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "h3.title"},
            {"è¦ç´ ": "ä¾¡æ ¼", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "span.product-price", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "div.cost"},
            {"è¦ç´ ": "è©•ä¾¡", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "div.product-rating", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "div.stars"},
            {"è¦ç´ ": "ã‚«ãƒ†ã‚´ãƒª", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "span.product-category", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "span.tag"},
            {"è¦ç´ ": "èª¬æ˜", "v1ã‚»ãƒ¬ã‚¯ã‚¿": "p.product-desc", "v2ã‚»ãƒ¬ã‚¯ã‚¿": "p.desc"},
        ])
        st.table(changes)

        # BS4 vs Scrapling æ¯”è¼ƒ
        st.subheader("BS4 vs Scraplingï¼ˆv2ã®HTMLã«v1ã‚»ãƒ¬ã‚¯ã‚¿ã‚’é©ç”¨ï¼‰")

        v2_bs4 = adaptive_data.get("v2_bs4", {})
        v2_scrapling = adaptive_data.get("v2_scrapling", {})

        comparison_rows = []
        for label in ["å•†å“å", "ä¾¡æ ¼", "è©•ä¾¡", "ã‚«ãƒ†ã‚´ãƒª", "èª¬æ˜"]:
            bs4_count = v2_bs4.get(label, 0)
            scr_data = v2_scrapling.get(label, {})
            scr_status = scr_data.get("status", "not_found") if isinstance(scr_data, dict) else "not_found"
            scr_text = scr_data.get("text", "-") if isinstance(scr_data, dict) else "-"
            comparison_rows.append({
                "ã‚»ãƒ¬ã‚¯ã‚¿": label,
                "BS4 (v2)": f"ğŸ’¥ {bs4_count}ä»¶" if bs4_count == 0 else f"âœ… {bs4_count}ä»¶",
                "Scrapling": f"âœ… {scr_text}" if scr_status == "restored" else "ğŸ’¥ å¾©å…ƒå¤±æ•—",
            })

        st.table(pd.DataFrame(comparison_rows))

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        col1, col2 = st.columns(2)
        bs4_total = sum(v for v in v2_bs4.values() if isinstance(v, int))
        scrapling_restored = sum(
            1 for v in v2_scrapling.values()
            if isinstance(v, dict) and v.get("status") == "restored"
        )

        with col1:
            st.metric("BS4 (v1ã‚»ãƒ¬ã‚¯ã‚¿ â†’ v2)", f"ğŸ’¥ {bs4_total}ä»¶", delta=None)
        with col2:
            st.metric("Scrapling Adaptive", f"âœ… {scrapling_restored}ä»¶å¾©å…ƒ", delta=None)

        # å¾©å…ƒè©³ç´°
        st.subheader("å¾©å…ƒè©³ç´°")
        for label, data in v2_scrapling.items():
            if isinstance(data, dict):
                with st.expander(f"{label} ({data.get('original_selector', '')})"):
                    st.json(data)


# ===== âš¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œãƒšãƒ¼ã‚¸ =====
elif page == "âš¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ":
    st.title("âš¡ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ")

    url = st.text_input("å¯¾è±¡URL", value="http://localhost:5001")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ•·ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"):
            with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­..."):
                result = subprocess.run(
                    ["python", "-m", "scraper.basic"],
                    capture_output=True, text=True, cwd=PROJECT_ROOT,
                )
                if result.returncode == 0:
                    st.success("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")
                    st.code(result.stdout)

                    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                    json_files = sorted(glob.glob(os.path.join(DATA_DIR, "products_*.json")))
                    if json_files:
                        latest = json_files[-1]
                        with open(latest, "r", encoding="utf-8") as f:
                            preview_data = json.load(f)
                        st.subheader("å–å¾—ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                        st.dataframe(pd.DataFrame(preview_data).head(5))
                else:
                    st.error("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼")
                    st.code(result.stderr)

    with col2:
        if st.button("ğŸ”„ Adaptive ãƒ•ãƒ«ãƒ‡ãƒ¢å®Ÿè¡Œ"):
            with st.spinner("Adaptive ãƒ•ãƒ«ãƒ‡ãƒ¢å®Ÿè¡Œä¸­..."):
                result = subprocess.run(
                    ["python", "-m", "scraper.adaptive", "full"],
                    capture_output=True, text=True, cwd=PROJECT_ROOT,
                )
                if result.returncode == 0:
                    st.success("Adaptive ãƒ•ãƒ«ãƒ‡ãƒ¢å®Œäº†ï¼")
                    st.code(result.stdout)
                else:
                    st.error("Adaptive ãƒ‡ãƒ¢ã‚¨ãƒ©ãƒ¼")
                    st.code(result.stderr)
